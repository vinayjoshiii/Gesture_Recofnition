{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gesture Recognition\n",
    "In this group project, you are going to build a 3D Conv model that will be able to predict the 5 gestures correctly. Please import the following libraries to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Dense, GRU, Flatten, TimeDistributed, BatchNormalization, Activation, Dropout, Conv3D, MaxPooling3D, Conv2D, MaxPooling2D, SimpleRNN, LSTM\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, Callback\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from skimage.transform import resize  # Use skimage for image resizing\n",
    "import imageio\n",
    "from imageio import imread  # Use imageio for reading images\n",
    "from PIL import Image\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the random seed so that the results don't vary drastically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(30)\n",
    "import random as rn\n",
    "rn.seed(30)\n",
    "from keras import backend as K\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block, you read the folder names for training and validation. You also set the `batch_size` here. Note that you set the batch size in such a way that you are able to use the GPU in full capacity. You keep increasing the batch size until the machine throws an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc = np.random.permutation(open('C:\\\\Users\\\\Vinay Joshi\\\\Documents\\\\PGD AI\\\\5. Gesture Recognition Project\\\\Project_data\\\\train.csv').readlines())\n",
    "val_doc = np.random.permutation(open('C:\\\\Users\\\\Vinay Joshi\\\\Documents\\\\PGD AI\\\\5. Gesture Recognition Project\\\\Project_data\\\\val.csv').readlines())\n",
    "\n",
    "train_path = 'C:\\\\Users\\\\Vinay Joshi\\\\Documents\\\\PGD AI\\\\5. Gesture Recognition Project\\\\Project_data\\\\train'\n",
    "val_path = 'C:\\\\Users\\\\Vinay Joshi\\\\Documents\\\\PGD AI\\\\5. Gesture Recognition Project\\\\Project_data\\\\val'\n",
    "\n",
    "# Define the model directory\n",
    "model_dir = 'C:\\\\Users\\\\Vinay Joshi\\\\Documents\\\\PGD AI\\\\5. Gesture Recognition Project\\\\Bestmodels'\n",
    "\n",
    "# Define filepaths for all models\n",
    "model_conv3D_filepath = model_dir + '\\\\' + 'BestModelConv3D.keras'\n",
    "model_2Drnn_filepath = model_dir + '\\\\' + 'BestModelConv2D_RNN.keras'\n",
    "model_2Dlstm_filepath = model_dir + '\\\\' + 'BestModelConv2D_LSTM.keras'\n",
    "model_2Dgru_filepath = model_dir + '\\\\' + 'BestModelConv2D_GRU.keras'\n",
    "model_3Drnn_filepath = model_dir + '\\\\' + 'BestModelConv3D_RNN.keras'\n",
    "model_3Dlstm_filepath = model_dir + '\\\\' + 'BestModelConv3D_LSTM.keras'\n",
    "model_3Dgru_filepath = model_dir + '\\\\' + 'BestModelConv3D_GRU.keras'\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 30 # choose the number of epochs\n",
    "\n",
    "# Defining input dimensions for the model\n",
    "num_frames = 16  # x, number of frames\n",
    "img_height = 64  # y, image height\n",
    "img_width = 64   # z, image width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "This is one of the most important part of the code. The overall structure of the generator has been given. In the generator, you are going to preprocess the images as you have images of 2 different dimensions as well as create a batch of video frames. You have to experiment with `img_idx`, `y`,`z` and normalization such that you get high accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generator(source_path, folder_list, batch_size, x, y, z):\n",
    "    print( 'Source path = ', source_path, '; batch size =', batch_size)\n",
    "    #img_idx = #create a list of image numbers you want to use for a particular video\n",
    "  \n",
    "    img_idx = np.linspace(0, 29, x, dtype=int)  # Select 10 frames from 30\n",
    "\n",
    "    while True:\n",
    "        t = np.random.permutation(folder_list)\n",
    "        num_batches = len(folder_list) // batch_size # calculate the number of batches\n",
    "        for batch in range(num_batches): # we iterate over the number of batches\n",
    "            batch_data = np.zeros((batch_size,x,y,z,3)) # x is the number of images you use for each video, (y,z) is the final size of the input images and 3 is the number of channels RGB\n",
    "            batch_labels = np.zeros((batch_size,5)) # batch_labels is the one hot representation of the output\n",
    "            for folder in range(batch_size): # iterate over the batch_size\n",
    "                imgs = os.listdir(source_path+'/'+ t[folder + (batch*batch_size)].split(';')[0]) # read all the images in the folder\n",
    "                for idx,item in enumerate(img_idx): #  Iterate iver the frames/images of a folder to read them in\n",
    "                    image = imread(source_path+'/'+ t[folder + (batch*batch_size)].strip().split(';')[0]+'/'+imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    #crop the images and resize them. Note that the images are of 2 different shape \n",
    "                    #and the conv3D will throw error if the inputs in a batch have different shapes\n",
    "\n",
    "                    # Crop the image\n",
    "                    h, w = image.shape[:2]\n",
    "                    min_dim = min(h, w)\n",
    "                    crop_img = image[(h//2 - min_dim//2):(h//2 + min_dim//2), (w//2 - min_dim//2):(w//2 + min_dim//2)]\n",
    "                    \n",
    "                    # Resize the image\n",
    "                    resized_image = resize(crop_img, (y, z), anti_aliasing=True)\n",
    "                    \n",
    "                    # Normalize the image\n",
    "                    resized_image /= 255.0                    \n",
    "                    \n",
    "                    batch_data[folder,idx,:,:,0] = resized_image[:, :, 0] #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,1] = resized_image[:, :, 1] #normalise and feed in the image\n",
    "                    batch_data[folder,idx,:,:,2] = resized_image[:, :, 2] #normalise and feed in the image\n",
    "                    \n",
    "                batch_labels[folder, int(t[folder + (batch*batch_size)].strip().split(';')[2])] = 1\n",
    "            yield batch_data, batch_labels #you yield the batch_data and the batch_labels, remember what does yield do\n",
    "\n",
    "        \n",
    "        # write the code for the remaining data points which are left after full batches\n",
    "        if len(folder_list) % batch_size != 0:\n",
    "            batch_data = np.zeros((len(folder_list) % batch_size, len(img_idx), y, z, 3))\n",
    "            batch_labels = np.zeros((len(folder_list) % batch_size, 5))\n",
    "            \n",
    "            for folder in range(len(folder_list) % batch_size):\n",
    "                imgs = os.listdir(source_path + '/' + t[folder + (num_batches * batch_size)].split(';')[0])\n",
    "                \n",
    "                for idx, item in enumerate(img_idx):\n",
    "                    image = imread(source_path + '/' + t[folder + (num_batches * batch_size)].strip().split(';')[0] + '/' + imgs[item]).astype(np.float32)\n",
    "                    \n",
    "                    # Crop the image\n",
    "                    h, w = image.shape[:2]\n",
    "                    min_dim = min(h, w)\n",
    "                    crop_img = image[(h//2 - min_dim//2):(h//2 + min_dim//2), (w//2 - min_dim//2):(w//2 + min_dim//2)]\n",
    "                    \n",
    "                    # Resize the image\n",
    "                    resized_image = resize(crop_img, (y, z), anti_aliasing=True)\n",
    "                    \n",
    "                    # Normalize the image\n",
    "                    resized_image /= 255.0\n",
    "                    \n",
    "                    # Store in batch_data\n",
    "                    batch_data[folder, idx, :, :, 0] = resized_image[:, :, 0]\n",
    "                    batch_data[folder, idx, :, :, 1] = resized_image[:, :, 1]\n",
    "                    batch_data[folder, idx, :, :, 2] = resized_image[:, :, 2]\n",
    "                \n",
    "                # One-hot encode labels\n",
    "                batch_labels[folder, int(t[folder + (num_batches * batch_size)].strip().split(';')[2])] = 1\n",
    "            \n",
    "            yield batch_data, batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note here that a video is represented above in the generator as (number of images, height, width, number of channels). Take this into consideration while creating the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# training sequences = 663\n",
      "# validation sequences = 100\n",
      "# epochs = 30\n"
     ]
    }
   ],
   "source": [
    "curr_dt_time = datetime.datetime.now()\n",
    "\n",
    "num_train_sequences = len(train_doc)\n",
    "print('# training sequences =', num_train_sequences)\n",
    "num_val_sequences = len(val_doc)\n",
    "print('# validation sequences =', num_val_sequences)\n",
    "print ('# epochs =', num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us create the `train_generator` and the `val_generator` which will be used in `.fit_generator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = generator(train_path, train_doc, batch_size, x=num_frames, y=img_height, z=img_width)\n",
    "val_generator = generator(val_path, val_doc, batch_size, x=num_frames, y=img_height, z=img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model directory if it doesn't exist\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `steps_per_epoch` and `validation_steps` are used by `fit_generator` to decide the number of next() calls it need to make."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (num_train_sequences%batch_size) == 0:\n",
    "    steps_per_epoch = int(num_train_sequences/batch_size)\n",
    "else:\n",
    "    steps_per_epoch = (num_train_sequences//batch_size) + 1\n",
    "\n",
    "if (num_val_sequences%batch_size) == 0:\n",
    "    validation_steps = int(num_val_sequences/batch_size)\n",
    "else:\n",
    "    validation_steps = (num_val_sequences//batch_size) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "Here you make the model using different functionalities that Keras provides. Remember to use `Conv3D` and `MaxPooling3D` and not `Conv2D` and `Maxpooling2D` for a 3D convolution model. You would want to use `TimeDistributed` while building a Conv2D + RNN model. Also remember that the last layer is the softmax. Design the network in such a way that the model is able to give good accuracy on the least number of parameters so that it can fit in the memory of the webcam."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "A basic Conv3D model with three 3D convolutional layers (32, 64, 128 filters) followed by fully connected layers. MaxPooling3D is applied after each convolution to reduce spatial dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Project_data\\train ; batch size = 32\n",
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.1721 - loss: 1.9838Source path =  C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Project_data\\val ; batch size = 32\n",
      "\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.23000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 4s/step - categorical_accuracy: 0.1731 - loss: 1.9734 - val_categorical_accuracy: 0.2300 - val_loss: 1.5611 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.2994 - loss: 1.5510\n",
      "Epoch 2: val_categorical_accuracy improved from 0.23000 to 0.44000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.2998 - loss: 1.5509 - val_categorical_accuracy: 0.4400 - val_loss: 1.4525 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.3893 - loss: 1.3645\n",
      "Epoch 3: val_categorical_accuracy improved from 0.44000 to 0.61000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.3906 - loss: 1.3611 - val_categorical_accuracy: 0.6100 - val_loss: 1.0322 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.5370 - loss: 1.0848\n",
      "Epoch 4: val_categorical_accuracy improved from 0.61000 to 0.70000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.5378 - loss: 1.0826 - val_categorical_accuracy: 0.7000 - val_loss: 0.8223 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.6530 - loss: 0.8416\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.70000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - categorical_accuracy: 0.6539 - loss: 0.8409 - val_categorical_accuracy: 0.6700 - val_loss: 0.8025 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.7273 - loss: 0.6775\n",
      "Epoch 6: val_categorical_accuracy improved from 0.70000 to 0.73000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - categorical_accuracy: 0.7282 - loss: 0.6763 - val_categorical_accuracy: 0.7300 - val_loss: 0.7139 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.7862 - loss: 0.5624\n",
      "Epoch 7: val_categorical_accuracy improved from 0.73000 to 0.83000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.7866 - loss: 0.5610 - val_categorical_accuracy: 0.8300 - val_loss: 0.6466 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.8255 - loss: 0.4417\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.8259 - loss: 0.4406 - val_categorical_accuracy: 0.8200 - val_loss: 0.5221 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.8800 - loss: 0.3218\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.8789 - loss: 0.3239 - val_categorical_accuracy: 0.8200 - val_loss: 0.4648 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9097 - loss: 0.2782\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - categorical_accuracy: 0.9093 - loss: 0.2773 - val_categorical_accuracy: 0.8000 - val_loss: 0.5686 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9312 - loss: 0.2073\n",
      "Epoch 11: val_categorical_accuracy improved from 0.83000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9310 - loss: 0.2071 - val_categorical_accuracy: 0.8700 - val_loss: 0.3862 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9565 - loss: 0.1653\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 3s/step - categorical_accuracy: 0.9559 - loss: 0.1663 - val_categorical_accuracy: 0.8600 - val_loss: 0.4422 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9531 - loss: 0.1465\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3s/step - categorical_accuracy: 0.9528 - loss: 0.1467 - val_categorical_accuracy: 0.8100 - val_loss: 0.5962 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9654 - loss: 0.1134\n",
      "Epoch 14: val_categorical_accuracy improved from 0.87000 to 0.88000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.9651 - loss: 0.1130 - val_categorical_accuracy: 0.8800 - val_loss: 0.5070 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9732 - loss: 0.0923\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.88000\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.9733 - loss: 0.0924 - val_categorical_accuracy: 0.8500 - val_loss: 0.5248 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9577 - loss: 0.0939\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - categorical_accuracy: 0.9581 - loss: 0.0934 - val_categorical_accuracy: 0.8800 - val_loss: 0.5077 - learning_rate: 2.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9941 - loss: 0.0350\n",
      "Epoch 17: val_categorical_accuracy improved from 0.88000 to 0.89000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - categorical_accuracy: 0.9941 - loss: 0.0351 - val_categorical_accuracy: 0.8900 - val_loss: 0.4780 - learning_rate: 2.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9868 - loss: 0.0393\n",
      "Epoch 18: val_categorical_accuracy improved from 0.89000 to 0.91000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D.keras\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.9868 - loss: 0.0391 - val_categorical_accuracy: 0.9100 - val_loss: 0.4670 - learning_rate: 2.0000e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9894 - loss: 0.0265\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.91000\n",
      "\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9895 - loss: 0.0264 - val_categorical_accuracy: 0.9000 - val_loss: 0.4526 - learning_rate: 2.0000e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9935 - loss: 0.0322\n",
      "Epoch 20: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9935 - loss: 0.0319 - val_categorical_accuracy: 0.9100 - val_loss: 0.4565 - learning_rate: 4.0000e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9999 - loss: 0.0157\n",
      "Epoch 21: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9998 - loss: 0.0160 - val_categorical_accuracy: 0.9000 - val_loss: 0.4628 - learning_rate: 4.0000e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0158\n",
      "Epoch 22: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0157 - val_categorical_accuracy: 0.9000 - val_loss: 0.4646 - learning_rate: 4.0000e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9958 - loss: 0.0198\n",
      "Epoch 23: val_categorical_accuracy did not improve from 0.91000\n",
      "\n",
      "Epoch 23: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9958 - loss: 0.0198 - val_categorical_accuracy: 0.9000 - val_loss: 0.4617 - learning_rate: 4.0000e-05\n",
      "Epoch 23: early stopping\n",
      "Restoring model weights from the end of the best epoch: 18.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x203eae096c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_conv3D = Sequential()\n",
    "\n",
    "# 1st Conv3D layer\n",
    "model_conv3D.add(Conv3D(filters=32, kernel_size=(3, 3, 3), \n",
    "                 input_shape=(num_frames, img_height, img_width, 3),  # Using variables\n",
    "                 padding='same', activation='relu'))\n",
    "model_conv3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# 2nd Conv3D layer\n",
    "model_conv3D.add(Conv3D(filters=64, kernel_size=(3, 3, 3), padding='same', activation='relu'))\n",
    "model_conv3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# 3rd Conv3D layer\n",
    "model_conv3D.add(Conv3D(filters=128, kernel_size=(3, 3, 3), padding='same', activation='relu'))\n",
    "model_conv3D.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Flatten the output for the Dense layer\n",
    "model_conv3D.add(Flatten())\n",
    "\n",
    "# Fully connected layer\n",
    "model_conv3D.add(Dense(512, activation='relu'))\n",
    "model_conv3D.add(Dropout(0.5))\n",
    "\n",
    "# Output layer (Softmax for classification)\n",
    "model_conv3D.add(Dense(5, activation='softmax'))  # 5 output classes\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint = ModelCheckpoint(model_conv3D_filepath, monitor='val_categorical_accuracy', verbose=1, save_best_only=True, \n",
    "                             save_weights_only=False, mode='max')  # Save as .keras\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_list = [checkpoint, LR, early_stopping]\n",
    "\n",
    "# Train the model\n",
    "model_conv3D.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "                 callbacks=callbacks_list, validation_data=val_generator, \n",
    "                 validation_steps=validation_steps, class_weight=None, initial_epoch=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "This model uses TimeDistributed Conv2D layers (32, 64 filters) for spatial feature extraction followed by a SimpleRNN with 64 units to handle temporal sequences. MaxPooling2D is applied after each Conv2D layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.2079 - loss: 1.6718\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.22000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 5s/step - categorical_accuracy: 0.2080 - loss: 1.6726 - val_categorical_accuracy: 0.2200 - val_loss: 1.6153 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.2234 - loss: 1.6650\n",
      "Epoch 2: val_categorical_accuracy did not improve from 0.22000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 0.2229 - loss: 1.6644 - val_categorical_accuracy: 0.2200 - val_loss: 1.5773 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.2770 - loss: 1.5649\n",
      "Epoch 3: val_categorical_accuracy improved from 0.22000 to 0.23000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.2771 - loss: 1.5648 - val_categorical_accuracy: 0.2300 - val_loss: 1.5256 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.3947 - loss: 1.4261\n",
      "Epoch 4: val_categorical_accuracy improved from 0.23000 to 0.59000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - categorical_accuracy: 0.3971 - loss: 1.4233 - val_categorical_accuracy: 0.5900 - val_loss: 1.2008 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.5759 - loss: 1.1241\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.59000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - categorical_accuracy: 0.5775 - loss: 1.1205 - val_categorical_accuracy: 0.5500 - val_loss: 1.0789 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.6621 - loss: 0.9203\n",
      "Epoch 6: val_categorical_accuracy improved from 0.59000 to 0.67000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.6639 - loss: 0.9158 - val_categorical_accuracy: 0.6700 - val_loss: 0.9463 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.7736 - loss: 0.6338\n",
      "Epoch 7: val_categorical_accuracy improved from 0.67000 to 0.73000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - categorical_accuracy: 0.7746 - loss: 0.6314 - val_categorical_accuracy: 0.7300 - val_loss: 0.7381 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.8458 - loss: 0.4218\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.73000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.8466 - loss: 0.4211 - val_categorical_accuracy: 0.6600 - val_loss: 0.9593 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9294 - loss: 0.2382\n",
      "Epoch 9: val_categorical_accuracy improved from 0.73000 to 0.77000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - categorical_accuracy: 0.9294 - loss: 0.2375 - val_categorical_accuracy: 0.7700 - val_loss: 0.6102 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9722 - loss: 0.1243\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.77000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - categorical_accuracy: 0.9718 - loss: 0.1244 - val_categorical_accuracy: 0.7400 - val_loss: 0.9441 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9800 - loss: 0.1075\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.77000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - categorical_accuracy: 0.9801 - loss: 0.1065 - val_categorical_accuracy: 0.7400 - val_loss: 0.9437 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9891 - loss: 0.0559\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.77000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.9889 - loss: 0.0561 - val_categorical_accuracy: 0.7700 - val_loss: 0.8586 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9838 - loss: 0.0439\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.77000\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - categorical_accuracy: 0.9842 - loss: 0.0434 - val_categorical_accuracy: 0.7500 - val_loss: 0.8096 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9989 - loss: 0.0162\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.77000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - categorical_accuracy: 0.9989 - loss: 0.0162 - val_categorical_accuracy: 0.7600 - val_loss: 0.9509 - learning_rate: 2.0000e-04\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20392287fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv2D + RNN Model\n",
    "model_rnn = Sequential()\n",
    "\n",
    "# TimeDistributed Conv2D layers\n",
    "model_rnn.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), \n",
    "                              input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_rnn.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_rnn.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')))\n",
    "model_rnn.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_rnn.add(TimeDistributed(Flatten()))  # Flatten before passing to RNN\n",
    "\n",
    "# SimpleRNN layer\n",
    "model_rnn.add(SimpleRNN(64, return_sequences=False))\n",
    "\n",
    "# Dense layers\n",
    "model_rnn.add(Dense(512, activation='relu'))\n",
    "model_rnn.add(Dropout(0.5))\n",
    "model_rnn.add(Dense(5, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_rnn = ModelCheckpoint(model_2Drnn_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_rnn_list = [checkpoint_rnn, LR, early_stopping_rnn]\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "model_rnn.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "              callbacks=callbacks_rnn_list, validation_data=val_generator, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "Combines TimeDistributed Conv2D layers (32, 64 filters) for spatial features with an LSTM layer (64 units) for temporal dependencies. MaxPooling2D is used after each Conv2D layer, and the output is flattened before passing to the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.2010 - loss: 1.6405\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.27000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 4s/step - categorical_accuracy: 0.2023 - loss: 1.6399 - val_categorical_accuracy: 0.2700 - val_loss: 1.5677 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.3125 - loss: 1.5425\n",
      "Epoch 2: val_categorical_accuracy improved from 0.27000 to 0.69000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - categorical_accuracy: 0.3146 - loss: 1.5406 - val_categorical_accuracy: 0.6900 - val_loss: 1.2790 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.6169 - loss: 1.1479\n",
      "Epoch 3: val_categorical_accuracy did not improve from 0.69000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m80s\u001b[0m 4s/step - categorical_accuracy: 0.6174 - loss: 1.1435 - val_categorical_accuracy: 0.5300 - val_loss: 1.1808 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.7184 - loss: 0.7985\n",
      "Epoch 4: val_categorical_accuracy improved from 0.69000 to 0.74000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.7199 - loss: 0.7944 - val_categorical_accuracy: 0.7400 - val_loss: 0.7835 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.8580 - loss: 0.4361\n",
      "Epoch 5: val_categorical_accuracy improved from 0.74000 to 0.79000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 0.8589 - loss: 0.4342 - val_categorical_accuracy: 0.7900 - val_loss: 0.6406 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9017 - loss: 0.2650\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.79000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.9021 - loss: 0.2644 - val_categorical_accuracy: 0.7600 - val_loss: 0.7122 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9630 - loss: 0.1527\n",
      "Epoch 7: val_categorical_accuracy improved from 0.79000 to 0.82000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - categorical_accuracy: 0.9632 - loss: 0.1511 - val_categorical_accuracy: 0.8200 - val_loss: 0.6374 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9920 - loss: 0.0412\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.82000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.9919 - loss: 0.0412 - val_categorical_accuracy: 0.7700 - val_loss: 0.7147 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9933 - loss: 0.0336\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.82000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 0.9930 - loss: 0.0340 - val_categorical_accuracy: 0.7500 - val_loss: 0.7673 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9915 - loss: 0.0316\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.82000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 4s/step - categorical_accuracy: 0.9915 - loss: 0.0317 - val_categorical_accuracy: 0.7400 - val_loss: 1.0859 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9919 - loss: 0.0205\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.82000\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - categorical_accuracy: 0.9920 - loss: 0.0204 - val_categorical_accuracy: 0.7700 - val_loss: 1.0212 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9994 - loss: 0.0080\n",
      "Epoch 12: val_categorical_accuracy improved from 0.82000 to 0.86000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 0.9994 - loss: 0.0080 - val_categorical_accuracy: 0.8600 - val_loss: 0.4424 - learning_rate: 2.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0030\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0030 - val_categorical_accuracy: 0.7600 - val_loss: 1.0145 - learning_rate: 2.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0035\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0035 - val_categorical_accuracy: 0.8000 - val_loss: 0.6676 - learning_rate: 2.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0021\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m86s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0021 - val_categorical_accuracy: 0.8000 - val_loss: 0.8660 - learning_rate: 2.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.86000\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0018 - val_categorical_accuracy: 0.8000 - val_loss: 0.7062 - learning_rate: 2.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 17: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0023 - val_categorical_accuracy: 0.8000 - val_loss: 0.8209 - learning_rate: 4.0000e-05\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x203b867e8f0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv2D + LSTM Model\n",
    "model_lstm = Sequential()\n",
    "\n",
    "# TimeDistributed Conv2D layers\n",
    "model_lstm.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), \n",
    "                               input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_lstm.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_lstm.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')))\n",
    "model_lstm.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_lstm.add(TimeDistributed(Flatten()))  # Flatten before passing to LSTM\n",
    "\n",
    "# LSTM layer\n",
    "model_lstm.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "# Dense layers\n",
    "model_lstm.add(Dense(512, activation='relu'))\n",
    "model_lstm.add(Dropout(0.5))\n",
    "model_lstm.add(Dense(5, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_lstm = ModelCheckpoint(model_2Dlstm_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                  save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping_lstm = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_lstm_list = [checkpoint_lstm, LR, early_stopping_lstm]\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "model_lstm.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "               callbacks=callbacks_lstm_list, validation_data=val_generator, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "This architecture uses TimeDistributed Conv2D layers (32, 64 filters) followed by a GRU layer with 64 units. MaxPooling2D is applied after each Conv2D layer, and the output is flattened before the GRU layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.2085 - loss: 1.6191\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.28000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 4s/step - categorical_accuracy: 0.2096 - loss: 1.6181 - val_categorical_accuracy: 0.2800 - val_loss: 1.4887 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.4015 - loss: 1.4391\n",
      "Epoch 2: val_categorical_accuracy improved from 0.28000 to 0.57000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 0.4040 - loss: 1.4356 - val_categorical_accuracy: 0.5700 - val_loss: 1.1469 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.5723 - loss: 1.1113\n",
      "Epoch 3: val_categorical_accuracy improved from 0.57000 to 0.60000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m79s\u001b[0m 4s/step - categorical_accuracy: 0.5744 - loss: 1.1076 - val_categorical_accuracy: 0.6000 - val_loss: 1.0671 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.7247 - loss: 0.7321\n",
      "Epoch 4: val_categorical_accuracy improved from 0.60000 to 0.74000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m88s\u001b[0m 4s/step - categorical_accuracy: 0.7259 - loss: 0.7296 - val_categorical_accuracy: 0.7400 - val_loss: 0.7711 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.8400 - loss: 0.4550\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.74000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - categorical_accuracy: 0.8407 - loss: 0.4529 - val_categorical_accuracy: 0.7100 - val_loss: 0.7784 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9008 - loss: 0.2789\n",
      "Epoch 6: val_categorical_accuracy improved from 0.74000 to 0.80000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 0.9015 - loss: 0.2770 - val_categorical_accuracy: 0.8000 - val_loss: 0.6479 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9453 - loss: 0.1598\n",
      "Epoch 7: val_categorical_accuracy improved from 0.80000 to 0.81000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 0.9460 - loss: 0.1580 - val_categorical_accuracy: 0.8100 - val_loss: 0.6187 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 0.9877 - loss: 0.0558\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.81000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 4s/step - categorical_accuracy: 0.9874 - loss: 0.0566 - val_categorical_accuracy: 0.8000 - val_loss: 0.7439 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0337\n",
      "Epoch 9: val_categorical_accuracy improved from 0.81000 to 0.84000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv2D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0332 - val_categorical_accuracy: 0.8400 - val_loss: 0.6963 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0068\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0067 - val_categorical_accuracy: 0.8000 - val_loss: 0.7461 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0043\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.84000\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m85s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0043 - val_categorical_accuracy: 0.8200 - val_loss: 0.7111 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0019 - val_categorical_accuracy: 0.8200 - val_loss: 0.7072 - learning_rate: 2.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0014\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0014 - val_categorical_accuracy: 0.8400 - val_loss: 0.6973 - learning_rate: 2.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m84s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0015 - val_categorical_accuracy: 0.8200 - val_loss: 0.7188 - learning_rate: 2.0000e-04\n",
      "Epoch 14: early stopping\n",
      "Restoring model weights from the end of the best epoch: 9.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x203ca63fbb0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv2D + GRU Model\n",
    "model_gru = Sequential()\n",
    "\n",
    "# TimeDistributed Conv2D layers\n",
    "model_gru.add(TimeDistributed(Conv2D(32, (3, 3), activation='relu', padding='same'), \n",
    "                              input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_gru.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_gru.add(TimeDistributed(Conv2D(64, (3, 3), activation='relu', padding='same')))\n",
    "model_gru.add(TimeDistributed(MaxPooling2D(pool_size=(2, 2))))\n",
    "model_gru.add(TimeDistributed(Flatten()))  # Flatten before passing to GRU\n",
    "\n",
    "# GRU layer\n",
    "model_gru.add(GRU(64, return_sequences=False))\n",
    "\n",
    "# Dense layers\n",
    "model_gru.add(Dense(512, activation='relu'))\n",
    "model_gru.add(Dropout(0.5))\n",
    "model_gru.add(Dense(5, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_gru = ModelCheckpoint(model_2Dgru_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping_gru = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_gru_list = [checkpoint_gru, LR, early_stopping_gru]\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "model_gru.fit(train_generator, steps_per_epoch=steps_per_epoch, epochs=num_epochs, verbose=1, \n",
    "              callbacks=callbacks_gru_list, validation_data=val_generator, validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5\n",
    "A Conv3D model with two 3D convolutional layers (32, 64 filters) followed by a SimpleRNN with 64 units. MaxPooling3D reduces spatial dimensions after each convolution, and the output is flattened before the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.2172 - loss: 1.7549\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.23000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - categorical_accuracy: 0.2161 - loss: 1.7537 - val_categorical_accuracy: 0.2300 - val_loss: 1.5981 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.2350 - loss: 1.5975\n",
      "Epoch 2: val_categorical_accuracy improved from 0.23000 to 0.43000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 3s/step - categorical_accuracy: 0.2373 - loss: 1.5947 - val_categorical_accuracy: 0.4300 - val_loss: 1.2847 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.4438 - loss: 1.2391\n",
      "Epoch 3: val_categorical_accuracy improved from 0.43000 to 0.62000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - categorical_accuracy: 0.4469 - loss: 1.2343 - val_categorical_accuracy: 0.6200 - val_loss: 0.9595 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.6145 - loss: 0.8818\n",
      "Epoch 4: val_categorical_accuracy improved from 0.62000 to 0.75000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.6164 - loss: 0.8788 - val_categorical_accuracy: 0.7500 - val_loss: 0.7178 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.7992 - loss: 0.5636\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.75000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.7993 - loss: 0.5629 - val_categorical_accuracy: 0.7500 - val_loss: 0.7074 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.8458 - loss: 0.4189\n",
      "Epoch 6: val_categorical_accuracy improved from 0.75000 to 0.81000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.8475 - loss: 0.4154 - val_categorical_accuracy: 0.8100 - val_loss: 0.4661 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9147 - loss: 0.2376\n",
      "Epoch 7: val_categorical_accuracy improved from 0.81000 to 0.83000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 0.9153 - loss: 0.2360 - val_categorical_accuracy: 0.8300 - val_loss: 0.4706 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9802 - loss: 0.0943\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.9797 - loss: 0.0950 - val_categorical_accuracy: 0.7700 - val_loss: 0.6884 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9606 - loss: 0.1000\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - categorical_accuracy: 0.9609 - loss: 0.0997 - val_categorical_accuracy: 0.8200 - val_loss: 0.6132 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9939 - loss: 0.0328\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.83000\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9938 - loss: 0.0329 - val_categorical_accuracy: 0.7900 - val_loss: 0.7083 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9953 - loss: 0.0328\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.9954 - loss: 0.0324 - val_categorical_accuracy: 0.8100 - val_loss: 0.4874 - learning_rate: 2.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0154\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.83000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0153 - val_categorical_accuracy: 0.8100 - val_loss: 0.5391 - learning_rate: 2.0000e-04\n",
      "Epoch 12: early stopping\n",
      "Restoring model weights from the end of the best epoch: 7.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x205ef61e740>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv3D + SimpleRNN Model\n",
    "model_conv3D_rnn = Sequential()\n",
    "\n",
    "# Conv3D layers\n",
    "model_conv3D_rnn.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                            input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_rnn.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_rnn.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model_conv3D_rnn.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# TimeDistributed(Flatten()) to keep the time dimension intact\n",
    "model_conv3D_rnn.add(TimeDistributed(Flatten()))  # Flatten the spatial dimensions but keep the time axis\n",
    "\n",
    "# SimpleRNN layer\n",
    "model_conv3D_rnn.add(SimpleRNN(64, return_sequences=False))\n",
    "\n",
    "# Fully connected layers\n",
    "model_conv3D_rnn.add(Dense(512, activation='relu'))\n",
    "model_conv3D_rnn.add(Dropout(0.5))\n",
    "model_conv3D_rnn.add(Dense(5, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_rnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_rnn = ModelCheckpoint(model_3Drnn_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping_rnn = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_rnn_list = [checkpoint_rnn, LR, early_stopping_rnn]\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "model_conv3D_rnn.fit(train_generator, \n",
    "                     steps_per_epoch=steps_per_epoch, \n",
    "                     epochs=num_epochs, \n",
    "                     verbose=1, \n",
    "                     callbacks=callbacks_rnn_list,  # Using ModelCheckpoint, ReduceLROnPlateau, and EarlyStopping callbacks\n",
    "                     validation_data=val_generator, \n",
    "                     validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6\n",
    "Combines Conv3D layers (32, 64 filters) for spatial-temporal feature extraction with an LSTM layer (64 units). MaxPooling3D is applied after each convolution, followed by a TimeDistributed Flatten layer and the LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.2163 - loss: 1.6203\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.28000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - categorical_accuracy: 0.2181 - loss: 1.6191 - val_categorical_accuracy: 0.2800 - val_loss: 1.4614 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.3392 - loss: 1.4849\n",
      "Epoch 2: val_categorical_accuracy improved from 0.28000 to 0.49000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.3433 - loss: 1.4805 - val_categorical_accuracy: 0.4900 - val_loss: 1.1892 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.5938 - loss: 1.0491\n",
      "Epoch 3: val_categorical_accuracy improved from 0.49000 to 0.66000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3s/step - categorical_accuracy: 0.5969 - loss: 1.0425 - val_categorical_accuracy: 0.6600 - val_loss: 0.7450 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.7826 - loss: 0.6068\n",
      "Epoch 4: val_categorical_accuracy improved from 0.66000 to 0.84000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.7825 - loss: 0.6046 - val_categorical_accuracy: 0.8400 - val_loss: 0.5474 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.8676 - loss: 0.4050\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.8684 - loss: 0.4027 - val_categorical_accuracy: 0.8100 - val_loss: 0.5247 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9455 - loss: 0.2029\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 0.9452 - loss: 0.2022 - val_categorical_accuracy: 0.8100 - val_loss: 0.5748 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9478 - loss: 0.1368\n",
      "Epoch 7: val_categorical_accuracy improved from 0.84000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9486 - loss: 0.1351 - val_categorical_accuracy: 0.8700 - val_loss: 0.3320 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9924 - loss: 0.0379\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - categorical_accuracy: 0.9923 - loss: 0.0379 - val_categorical_accuracy: 0.8200 - val_loss: 0.6535 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9911 - loss: 0.0420\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.9909 - loss: 0.0426 - val_categorical_accuracy: 0.8400 - val_loss: 0.5573 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9913 - loss: 0.0428\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 0.9916 - loss: 0.0419 - val_categorical_accuracy: 0.8400 - val_loss: 0.5488 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9946 - loss: 0.0212\n",
      "Epoch 11: val_categorical_accuracy improved from 0.87000 to 0.88000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM.keras\n",
      "\n",
      "Epoch 11: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 0.9947 - loss: 0.0208 - val_categorical_accuracy: 0.8800 - val_loss: 0.5947 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0114\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0111 - val_categorical_accuracy: 0.8600 - val_loss: 0.5356 - learning_rate: 2.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0032\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0032 - val_categorical_accuracy: 0.8700 - val_loss: 0.5817 - learning_rate: 2.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0015 - val_categorical_accuracy: 0.8600 - val_loss: 0.5620 - learning_rate: 2.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0017\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.88000\n",
      "\n",
      "Epoch 15: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0017 - val_categorical_accuracy: 0.8600 - val_loss: 0.5505 - learning_rate: 2.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0012\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0012 - val_categorical_accuracy: 0.8700 - val_loss: 0.4463 - learning_rate: 4.0000e-05\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x205ed316e30>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv3D + LSTM Model\n",
    "model_conv3D_lstm = Sequential()\n",
    "\n",
    "# Conv3D layers\n",
    "model_conv3D_lstm.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                             input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_lstm.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_lstm.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model_conv3D_lstm.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# TimeDistributed(Flatten()) to keep the time dimension intact\n",
    "model_conv3D_lstm.add(TimeDistributed(Flatten()))  # Flatten the spatial dimensions but keep the time axis\n",
    "\n",
    "# LSTM layer\n",
    "model_conv3D_lstm.add(LSTM(64, return_sequences=False))\n",
    "\n",
    "# Fully connected layers\n",
    "model_conv3D_lstm.add(Dense(512, activation='relu'))\n",
    "model_conv3D_lstm.add(Dropout(0.5))\n",
    "model_conv3D_lstm.add(Dense(5, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_lstm.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_lstm = ModelCheckpoint(model_3Dlstm_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                  save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping_lstm = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_lstm_list = [checkpoint_lstm, LR, early_stopping_lstm]\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "model_conv3D_lstm.fit(train_generator, \n",
    "                      steps_per_epoch=steps_per_epoch, \n",
    "                      epochs=num_epochs, \n",
    "                      verbose=1, \n",
    "                      callbacks=callbacks_lstm_list, \n",
    "                      validation_data=val_generator, \n",
    "                      validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 7\n",
    "Similar to Conv3D + LSTM, this model uses Conv3D layers (32, 64 filters) but replaces LSTM with a GRU layer (64 units). MaxPooling3D is applied after each convolution, followed by a TimeDistributed Flatten layer and the GRU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.1967 - loss: 1.6712\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.31000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - categorical_accuracy: 0.1975 - loss: 1.6701 - val_categorical_accuracy: 0.3100 - val_loss: 1.5180 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.3314 - loss: 1.4977\n",
      "Epoch 2: val_categorical_accuracy improved from 0.31000 to 0.47000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.3341 - loss: 1.4936 - val_categorical_accuracy: 0.4700 - val_loss: 1.2234 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.5182 - loss: 1.1237\n",
      "Epoch 3: val_categorical_accuracy improved from 0.47000 to 0.65000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 3s/step - categorical_accuracy: 0.5217 - loss: 1.1181 - val_categorical_accuracy: 0.6500 - val_loss: 0.8205 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.7052 - loss: 0.7474\n",
      "Epoch 4: val_categorical_accuracy improved from 0.65000 to 0.71000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.7065 - loss: 0.7448 - val_categorical_accuracy: 0.7100 - val_loss: 0.6919 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.7880 - loss: 0.5958\n",
      "Epoch 5: val_categorical_accuracy improved from 0.71000 to 0.84000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 3s/step - categorical_accuracy: 0.7898 - loss: 0.5908 - val_categorical_accuracy: 0.8400 - val_loss: 0.5162 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.8955 - loss: 0.3025\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.8965 - loss: 0.3008 - val_categorical_accuracy: 0.8100 - val_loss: 0.6078 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9208 - loss: 0.2263\n",
      "Epoch 7: val_categorical_accuracy improved from 0.84000 to 0.86000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 0.9218 - loss: 0.2240 - val_categorical_accuracy: 0.8600 - val_loss: 0.6221 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9679 - loss: 0.0850\n",
      "Epoch 8: val_categorical_accuracy improved from 0.86000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 0.9684 - loss: 0.0843 - val_categorical_accuracy: 0.8700 - val_loss: 0.5117 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 0.9889 - loss: 0.0338\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 0.9891 - loss: 0.0336 - val_categorical_accuracy: 0.8000 - val_loss: 0.6015 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0155\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0154 - val_categorical_accuracy: 0.8700 - val_loss: 0.5637 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0058\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0058 - val_categorical_accuracy: 0.8500 - val_loss: 0.6269 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0038\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.87000\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m63s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0038 - val_categorical_accuracy: 0.8400 - val_loss: 0.7411 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0031\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0031 - val_categorical_accuracy: 0.8500 - val_loss: 0.4902 - learning_rate: 2.0000e-04\n",
      "Epoch 13: early stopping\n",
      "Restoring model weights from the end of the best epoch: 8.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x205ec7cebf0>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv3D + GRU Model\n",
    "model_conv3D_gru = Sequential()\n",
    "\n",
    "# Conv3D layers\n",
    "model_conv3D_gru.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                            input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_gru.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_gru.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model_conv3D_gru.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# TimeDistributed(Flatten()) to keep the time dimension intact\n",
    "model_conv3D_gru.add(TimeDistributed(Flatten()))  # Flatten the spatial dimensions but keep the time axis\n",
    "\n",
    "# GRU layer (expects 3D input: (batch_size, timesteps, features))\n",
    "model_conv3D_gru.add(GRU(64, return_sequences=False))\n",
    "\n",
    "# Fully connected layers\n",
    "model_conv3D_gru.add(Dense(512, activation='relu'))\n",
    "model_conv3D_gru.add(Dropout(0.5))\n",
    "model_conv3D_gru.add(Dense(5, activation='softmax'))  # Output layer\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_gru.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_gru = ModelCheckpoint(model_3Dgru_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                 save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Define ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add EarlyStopping to monitor val_categorical_accuracy\n",
    "early_stopping_gru = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Add all callbacks to the list\n",
    "callbacks_gru_list = [checkpoint_gru, LR, early_stopping_gru]\n",
    "\n",
    "# Train the model with EarlyStopping\n",
    "model_conv3D_gru.fit(train_generator, \n",
    "                     steps_per_epoch=steps_per_epoch, \n",
    "                     epochs=num_epochs, \n",
    "                     verbose=1, \n",
    "                     callbacks=callbacks_gru_list, \n",
    "                     validation_data=val_generator, \n",
    "                     validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's evaluate the models we've built till now on test data and compare thier soze, time to test and test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - categorical_accuracy: 1.0000 - loss: 0.0090\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - categorical_accuracy: 0.9379 - loss: 0.3880  \n",
      "Model: Conv3D\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9100000262260437\n",
      "Model Size: 99 MB\n",
      "Test Inference Time: 10.2 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - categorical_accuracy: 0.9923 - loss: 0.0654\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1s/step - categorical_accuracy: 0.7563 - loss: 0.7149 \n",
      "Model: Conv2D + RNN\n",
      "Train Accuracy: 0.9894419312477112\n",
      "Test Accuracy: 0.75\n",
      "Model Size: 12 MB\n",
      "Test Inference Time: 13.1 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0028\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - categorical_accuracy: 0.7592 - loss: 0.9054  \n",
      "Model: Conv2D + LSTM\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.75\n",
      "Model Size: 48 MB\n",
      "Test Inference Time: 12.9 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - categorical_accuracy: 0.9973 - loss: 0.0104\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - categorical_accuracy: 0.8771 - loss: 0.7072 \n",
      "Model: Conv2D + GRU\n",
      "Train Accuracy: 0.9969834089279175\n",
      "Test Accuracy: 0.8399999737739563\n",
      "Model Size: 36 MB\n",
      "Test Inference Time: 12.6 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m51s\u001b[0m 3s/step - categorical_accuracy: 0.9895 - loss: 0.0850\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2s/step - categorical_accuracy: 0.8123 - loss: 0.4617 \n",
      "Model: Conv3D + RNN\n",
      "Train Accuracy: 0.9849170446395874\n",
      "Test Accuracy: 0.8199999928474426\n",
      "Model Size: 13 MB\n",
      "Test Inference Time: 10.1 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 2s/step - categorical_accuracy: 1.0000 - loss: 0.0080\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - categorical_accuracy: 0.8822 - loss: 0.5725 \n",
      "Model: Conv3D + LSTM\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.8799999952316284\n",
      "Model Size: 49 MB\n",
      "Test Inference Time: 12.4 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0265\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step - categorical_accuracy: 0.8441 - loss: 0.5326  \n",
      "Model: Conv3D + GRU\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.8399999737739563\n",
      "Model Size: 37 MB\n",
      "Test Inference Time: 13.5 sec\n",
      "==================================================\n",
      "\n",
      "Summary of Model Performance:\n",
      "\n",
      "           Model  Train Acc  Test Acc  Model Size (MB) Test Time (sec)\n",
      "0         Conv3D   1.000000      0.91               99            10.2\n",
      "1   Conv2D + RNN   0.989442      0.75               12            13.1\n",
      "2  Conv2D + LSTM   1.000000      0.75               48            12.9\n",
      "3   Conv2D + GRU   0.996983      0.84               36            12.6\n",
      "4   Conv3D + RNN   0.984917      0.82               13            10.1\n",
      "5  Conv3D + LSTM   1.000000      0.88               49            12.4\n",
      "6   Conv3D + GRU   1.000000      0.84               37            13.5\n"
     ]
    }
   ],
   "source": [
    "# Dictionary to store the model names, accuracies, model sizes, and inference times\n",
    "model_accuracy_summary = {\n",
    "    'Model': [],\n",
    "    'Train Acc': [],\n",
    "    'Test Acc': [],\n",
    "    'Model Size (MB)': [],\n",
    "    'Test Time (sec)': []\n",
    "}\n",
    "\n",
    "# Calculate the number of test sequences and steps\n",
    "num_test_sequences = len(val_doc)\n",
    "if (num_test_sequences % batch_size) == 0:\n",
    "    test_steps = int(num_test_sequences / batch_size)\n",
    "else:\n",
    "    test_steps = (num_test_sequences // batch_size) + 1\n",
    "    \n",
    "# Function to evaluate the model and measure inference time on test data\n",
    "def evaluate_model(model_filepath, model_name):\n",
    "    # Load the model\n",
    "    model = load_model(model_filepath)\n",
    "\n",
    "    # Evaluate on training data\n",
    "    train_loss, train_accuracy = model.evaluate(train_generator, steps=steps_per_epoch, verbose=1)\n",
    "\n",
    "    # Measure inference time on test data\n",
    "    start_time = time.time()\n",
    "    test_loss, test_accuracy = model.evaluate(val_generator, steps=test_steps, verbose=1)\n",
    "    end_time = time.time()\n",
    "\n",
    "    # Calculate inference time\n",
    "    inference_time = end_time - start_time\n",
    "\n",
    "    # Calculate the model size in MB (formatted without decimals)\n",
    "    model_size = int(os.path.getsize(model_filepath) / (1024 * 1024))  # Convert from bytes to MB\n",
    "\n",
    "    # Append the results to the summary dictionary\n",
    "    model_accuracy_summary['Model'].append(model_name)\n",
    "    model_accuracy_summary['Train Acc'].append(train_accuracy)\n",
    "    model_accuracy_summary['Test Acc'].append(test_accuracy)\n",
    "    model_accuracy_summary['Model Size (MB)'].append(model_size)\n",
    "    model_accuracy_summary['Test Time (sec)'].append(f\"{inference_time:.1f}\")  # Format to 1 digit\n",
    "\n",
    "    # Print the results for the current model\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Train Accuracy: {train_accuracy}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy}\")\n",
    "    print(f\"Model Size: {model_size} MB\")  # No digits after the decimal point\n",
    "    print(f\"Test Inference Time: {inference_time:.1f} sec\")  # 1 digit after decimal point\n",
    "    print(\"=\"*50)\n",
    "\n",
    "\n",
    "# Evaluate the Conv3D model\n",
    "evaluate_model(model_conv3D_filepath, 'Conv3D')\n",
    "\n",
    "# Evaluate the Conv2D + RNN (SimpleRNN) model\n",
    "evaluate_model(model_2Drnn_filepath, 'Conv2D + RNN')\n",
    "\n",
    "# Evaluate the Conv2D + LSTM model\n",
    "evaluate_model(model_2Dlstm_filepath, 'Conv2D + LSTM')\n",
    "\n",
    "# Evaluate the Conv2D + GRU model\n",
    "evaluate_model(model_2Dgru_filepath, 'Conv2D + GRU')\n",
    "\n",
    "# Evaluate the Conv3D + RNN (SimpleRNN) model\n",
    "evaluate_model(model_3Drnn_filepath, 'Conv3D + RNN')\n",
    "\n",
    "# Evaluate the Conv3D + LSTM model\n",
    "evaluate_model(model_3Dlstm_filepath, 'Conv3D + LSTM')\n",
    "\n",
    "# Evaluate the Conv3D + GRU model\n",
    "evaluate_model(model_3Dgru_filepath, 'Conv3D + GRU')\n",
    "\n",
    "# Print the summary in a table format using pandas\n",
    "summary_df = pd.DataFrame(model_accuracy_summary)\n",
    "print(\"\\nSummary of Model Performance:\\n\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the first 7 models, I evaluated their performance based on three primary factors: test accuracy, model size, and inference time. The goal was to select models that could offer a good balance between these metrics, keeping in mind the constraints of deploying the model in a real-time environment like a webcam, where time to test and model size are crucial considerations.\n",
    "\n",
    "Out of the initial 7 models, the Conv3D + RNN, Conv3D + LSTM, and Conv3D + GRU models stood out as candidates for further experimentation. These models offered higher accuracy compared to their 2D counterparts and demonstrated better handling of spatiotemporal data. They also featured relatively small sizes and inference times, making them suitable for real-time use. We selected these three models because:\n",
    "\n",
    "They provided a good combination of accuracy and efficiency.\n",
    "They leveraged 3D convolutions, which are better suited for video data.\n",
    "The RNN, LSTM, and GRU units handled temporal dependencies effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defines the path for saving the models\n",
    "model_rnn_extra_filepath = model_dir + '\\\\' + 'BestModelConv3D_RNN_Extra.keras'\n",
    "model_lstm_extra_filepath = model_dir + '\\\\' + 'BestModelConv3D_LSTM_Extra.keras'\n",
    "model_gru_extra_filepath = model_dir + '\\\\' + 'BestModelConv3D_GRU_Extra.keras'\n",
    "model_rnn_optimized1_filepath = model_dir + '\\\\' + 'BestModelConv3D_RNN_Optimized2.keras'\n",
    "model_rnn_optimized2_filepath = model_dir + '\\\\' + 'BestModelConv3D_RNN_Optimized1.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# Defining input dimensions for the model\n",
    "num_frames = 16  # x, number of frames\n",
    "\n",
    "#Increase the image size\n",
    "img_height = 128  # y, image height\n",
    "img_width = 128   # z, image width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate the train and validation generators againto accomodate for the increase in the input image size\n",
    "train_generator = generator(source_path=train_path, folder_list=train_doc, batch_size=batch_size, x=num_frames, y=img_height, z=img_width)\n",
    "val_generator = generator(source_path=val_path, folder_list=val_doc, batch_size=batch_size, x=num_frames, y=img_height, z=img_width)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 8\n",
    "This model adds an additional Conv3D layer (64 filters) to the Conv3D + RNN architecture. The RNN layer has 128 units, and MaxPooling3D is applied after each convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source path =  C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Project_data\\train ; batch size = 32\n",
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.2293 - loss: 1.6375Source path =  C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Project_data\\val ; batch size = 32\n",
      "\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.24000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 6s/step - categorical_accuracy: 0.2307 - loss: 1.6357 - val_categorical_accuracy: 0.2400 - val_loss: 1.4376 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.4419 - loss: 1.3390\n",
      "Epoch 2: val_categorical_accuracy improved from 0.24000 to 0.63000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 0.4449 - loss: 1.3321 - val_categorical_accuracy: 0.6300 - val_loss: 0.7986 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.7511 - loss: 0.6897\n",
      "Epoch 3: val_categorical_accuracy improved from 0.63000 to 0.80000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 6s/step - categorical_accuracy: 0.7516 - loss: 0.6887 - val_categorical_accuracy: 0.8000 - val_loss: 0.5265 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8298 - loss: 0.4720\n",
      "Epoch 4: val_categorical_accuracy improved from 0.80000 to 0.83000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.8304 - loss: 0.4708 - val_categorical_accuracy: 0.8300 - val_loss: 0.5026 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.8723 - loss: 0.3488\n",
      "Epoch 5: val_categorical_accuracy improved from 0.83000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.8723 - loss: 0.3484 - val_categorical_accuracy: 0.8700 - val_loss: 0.4121 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.9333 - loss: 0.1986\n",
      "Epoch 6: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 6s/step - categorical_accuracy: 0.9332 - loss: 0.1988 - val_categorical_accuracy: 0.8600 - val_loss: 0.4337 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9544 - loss: 0.1213\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 6s/step - categorical_accuracy: 0.9545 - loss: 0.1216 - val_categorical_accuracy: 0.8100 - val_loss: 0.5194 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9692 - loss: 0.0991\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - categorical_accuracy: 0.9694 - loss: 0.0982 - val_categorical_accuracy: 0.8300 - val_loss: 0.5029 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9787 - loss: 0.0572\n",
      "Epoch 9: val_categorical_accuracy improved from 0.87000 to 0.88000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\n",
      "Epoch 9: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m136s\u001b[0m 6s/step - categorical_accuracy: 0.9789 - loss: 0.0568 - val_categorical_accuracy: 0.8800 - val_loss: 0.4895 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9947 - loss: 0.0380\n",
      "Epoch 10: val_categorical_accuracy improved from 0.88000 to 0.90000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - categorical_accuracy: 0.9948 - loss: 0.0373 - val_categorical_accuracy: 0.9000 - val_loss: 0.4213 - learning_rate: 2.0000e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0092\n",
      "Epoch 11: val_categorical_accuracy improved from 0.90000 to 0.92000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0093 - val_categorical_accuracy: 0.9200 - val_loss: 0.4244 - learning_rate: 2.0000e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0069\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.92000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0068 - val_categorical_accuracy: 0.9000 - val_loss: 0.4355 - learning_rate: 2.0000e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 1.0000 - loss: 0.0053\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.92000\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0053 - val_categorical_accuracy: 0.8900 - val_loss: 0.4482 - learning_rate: 2.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 1.0000 - loss: 0.0041\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.92000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0041 - val_categorical_accuracy: 0.8900 - val_loss: 0.4427 - learning_rate: 4.0000e-05\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0045\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.92000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0045 - val_categorical_accuracy: 0.9000 - val_loss: 0.4391 - learning_rate: 4.0000e-05\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0062\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.92000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0061 - val_categorical_accuracy: 0.9100 - val_loss: 0.4402 - learning_rate: 4.0000e-05\n",
      "Epoch 16: early stopping\n",
      "Restoring model weights from the end of the best epoch: 11.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x205ed330c10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv3D + RNN Model with an additional Conv3D layer\n",
    "model_conv3D_rnn_extra = Sequential()\n",
    "\n",
    "# Conv3D layers\n",
    "model_conv3D_rnn_extra.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                                  input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_rnn_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_rnn_extra.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model_conv3D_rnn_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Additional Conv3D layer\n",
    "model_conv3D_rnn_extra.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))  # Extra layer\n",
    "model_conv3D_rnn_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Reshape for RNN layer\n",
    "model_conv3D_rnn_extra.add(Reshape((-1, 16 * 16 * 64)))  # Adjust to new pooling result\n",
    "\n",
    "# RNN layer\n",
    "model_conv3D_rnn_extra.add(SimpleRNN(128, return_sequences=False))\n",
    "\n",
    "# Fully connected layers\n",
    "model_conv3D_rnn_extra.add(Dense(512, activation='relu'))\n",
    "model_conv3D_rnn_extra.add(Dropout(0.5))\n",
    "model_conv3D_rnn_extra.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_rnn_extra.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_rnn_extra = ModelCheckpoint(model_rnn_extra_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                       save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# EarlyStopping callback to stop training when val_categorical_accuracy plateaus\n",
    "early_stopping_rnn_extra = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add callbacks to the list\n",
    "callbacks_rnn_extra_list = [checkpoint_rnn_extra, early_stopping_rnn_extra, LR]\n",
    "\n",
    "# Train the model\n",
    "model_conv3D_rnn_extra.fit(train_generator, \n",
    "                           steps_per_epoch=steps_per_epoch, \n",
    "                           epochs=num_epochs, \n",
    "                           verbose=1, \n",
    "                           callbacks=callbacks_rnn_extra_list, \n",
    "                           validation_data=val_generator, \n",
    "                           validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 9\n",
    "Adds an extra Conv3D layer (64 filters) to the Conv3D + LSTM architecture. The LSTM layer has 128 units, and MaxPooling3D is applied after each convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.1935 - loss: 1.6228\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.17000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 6s/step - categorical_accuracy: 0.1942 - loss: 1.6223 - val_categorical_accuracy: 0.1700 - val_loss: 1.5801 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.2871 - loss: 1.5326\n",
      "Epoch 2: val_categorical_accuracy improved from 0.17000 to 0.43000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.2895 - loss: 1.5293 - val_categorical_accuracy: 0.4300 - val_loss: 1.2129 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.4256 - loss: 1.2430\n",
      "Epoch 3: val_categorical_accuracy improved from 0.43000 to 0.63000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.4287 - loss: 1.2382 - val_categorical_accuracy: 0.6300 - val_loss: 0.8741 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.5879 - loss: 0.9782\n",
      "Epoch 4: val_categorical_accuracy improved from 0.63000 to 0.72000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - categorical_accuracy: 0.5888 - loss: 0.9754 - val_categorical_accuracy: 0.7200 - val_loss: 0.7500 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.7745 - loss: 0.6274\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.72000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 6s/step - categorical_accuracy: 0.7733 - loss: 0.6290 - val_categorical_accuracy: 0.6800 - val_loss: 0.6752 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.7980 - loss: 0.5096\n",
      "Epoch 6: val_categorical_accuracy improved from 0.72000 to 0.76000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 6s/step - categorical_accuracy: 0.7992 - loss: 0.5077 - val_categorical_accuracy: 0.7600 - val_loss: 0.5402 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8455 - loss: 0.3604\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.76000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m132s\u001b[0m 6s/step - categorical_accuracy: 0.8466 - loss: 0.3585 - val_categorical_accuracy: 0.7500 - val_loss: 0.6074 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9558 - loss: 0.1673\n",
      "Epoch 8: val_categorical_accuracy improved from 0.76000 to 0.84000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m129s\u001b[0m 6s/step - categorical_accuracy: 0.9559 - loss: 0.1664 - val_categorical_accuracy: 0.8400 - val_loss: 0.4755 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9775 - loss: 0.0767\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 6s/step - categorical_accuracy: 0.9777 - loss: 0.0758 - val_categorical_accuracy: 0.8000 - val_loss: 0.8671 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9739 - loss: 0.0910\n",
      "Epoch 10: val_categorical_accuracy improved from 0.84000 to 0.86000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m134s\u001b[0m 6s/step - categorical_accuracy: 0.9743 - loss: 0.0900 - val_categorical_accuracy: 0.8600 - val_loss: 0.5826 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7s/step - categorical_accuracy: 0.9947 - loss: 0.0256\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 7s/step - categorical_accuracy: 0.9947 - loss: 0.0256 - val_categorical_accuracy: 0.8400 - val_loss: 0.5625 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9916 - loss: 0.0326\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.86000\n",
      "\n",
      "Epoch 12: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m133s\u001b[0m 6s/step - categorical_accuracy: 0.9915 - loss: 0.0328 - val_categorical_accuracy: 0.7800 - val_loss: 0.7959 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9939 - loss: 0.0272\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - categorical_accuracy: 0.9939 - loss: 0.0271 - val_categorical_accuracy: 0.8100 - val_loss: 0.6431 - learning_rate: 2.0000e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0074\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.86000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0073 - val_categorical_accuracy: 0.8500 - val_loss: 0.7092 - learning_rate: 2.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0033\n",
      "Epoch 15: val_categorical_accuracy improved from 0.86000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0033 - val_categorical_accuracy: 0.8700 - val_loss: 0.5207 - learning_rate: 2.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0024\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.87000\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0024 - val_categorical_accuracy: 0.8300 - val_loss: 0.7950 - learning_rate: 2.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0023\n",
      "Epoch 17: val_categorical_accuracy improved from 0.87000 to 0.88000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0023 - val_categorical_accuracy: 0.8800 - val_loss: 0.5208 - learning_rate: 4.0000e-05\n",
      "Epoch 18/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0022\n",
      "Epoch 18: val_categorical_accuracy did not improve from 0.88000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0022 - val_categorical_accuracy: 0.8500 - val_loss: 0.7222 - learning_rate: 4.0000e-05\n",
      "Epoch 19/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0019\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0019 - val_categorical_accuracy: 0.8500 - val_loss: 0.6915 - learning_rate: 4.0000e-05\n",
      "Epoch 20/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 20: val_categorical_accuracy improved from 0.88000 to 0.91000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_LSTM_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0020 - val_categorical_accuracy: 0.9100 - val_loss: 0.4325 - learning_rate: 4.0000e-05\n",
      "Epoch 21/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 21: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0018 - val_categorical_accuracy: 0.8700 - val_loss: 0.6795 - learning_rate: 4.0000e-05\n",
      "Epoch 22/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0020\n",
      "Epoch 22: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0020 - val_categorical_accuracy: 0.8300 - val_loss: 0.8189 - learning_rate: 4.0000e-05\n",
      "Epoch 23/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 23: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0018 - val_categorical_accuracy: 0.8900 - val_loss: 0.4952 - learning_rate: 4.0000e-05\n",
      "Epoch 24/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0015\n",
      "Epoch 24: val_categorical_accuracy did not improve from 0.91000\n",
      "\n",
      "Epoch 24: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0016 - val_categorical_accuracy: 0.8600 - val_loss: 0.6217 - learning_rate: 4.0000e-05\n",
      "Epoch 25/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0018\n",
      "Epoch 25: val_categorical_accuracy did not improve from 0.91000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.0018 - val_categorical_accuracy: 0.8700 - val_loss: 0.5723 - learning_rate: 8.0000e-06\n",
      "Epoch 25: early stopping\n",
      "Restoring model weights from the end of the best epoch: 20.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20603d54a00>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv3D + LSTM Model with an additional Conv3D layer\n",
    "model_conv3D_lstm_extra = Sequential()\n",
    "\n",
    "# Conv3D layers\n",
    "model_conv3D_lstm_extra.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                                   input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_lstm_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_lstm_extra.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model_conv3D_lstm_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Additional Conv3D layer\n",
    "model_conv3D_lstm_extra.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))  # Extra layer\n",
    "model_conv3D_lstm_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Reshape for LSTM layer\n",
    "model_conv3D_lstm_extra.add(Reshape((-1, 16 * 16 * 64)))  # Adjust to new pooling result\n",
    "\n",
    "# LSTM layer\n",
    "model_conv3D_lstm_extra.add(LSTM(128, return_sequences=False))\n",
    "\n",
    "# Fully connected layers\n",
    "model_conv3D_lstm_extra.add(Dense(512, activation='relu'))\n",
    "model_conv3D_lstm_extra.add(Dropout(0.5))\n",
    "model_conv3D_lstm_extra.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_lstm_extra.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_lstm_extra = ModelCheckpoint(model_lstm_extra_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                        save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# EarlyStopping callback to stop training when val_categorical_accuracy plateaus\n",
    "early_stopping_lstm_extra = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add callbacks to the list\n",
    "callbacks_lstm_extra_list = [checkpoint_lstm_extra, early_stopping_lstm_extra, LR]\n",
    "\n",
    "# Train the model\n",
    "model_conv3D_lstm_extra.fit(train_generator, \n",
    "                            steps_per_epoch=steps_per_epoch, \n",
    "                            epochs=num_epochs, \n",
    "                            verbose=1, \n",
    "                            callbacks=callbacks_lstm_extra_list, \n",
    "                            validation_data=val_generator, \n",
    "                            validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 10\n",
    "Extends the Conv3D + GRU model by adding another Conv3D layer (64 filters). The GRU layer has 128 units, and MaxPooling3D is applied after each convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.1991 - loss: 1.7352\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.21000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m131s\u001b[0m 6s/step - categorical_accuracy: 0.1993 - loss: 1.7316 - val_categorical_accuracy: 0.2100 - val_loss: 1.5508 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.2701 - loss: 1.4911\n",
      "Epoch 2: val_categorical_accuracy improved from 0.21000 to 0.39000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.2727 - loss: 1.4887 - val_categorical_accuracy: 0.3900 - val_loss: 1.2751 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.4664 - loss: 1.2173\n",
      "Epoch 3: val_categorical_accuracy improved from 0.39000 to 0.59000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 6s/step - categorical_accuracy: 0.4696 - loss: 1.2108 - val_categorical_accuracy: 0.5900 - val_loss: 0.9460 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.6830 - loss: 0.7851\n",
      "Epoch 4: val_categorical_accuracy improved from 0.59000 to 0.75000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 6s/step - categorical_accuracy: 0.6843 - loss: 0.7820 - val_categorical_accuracy: 0.7500 - val_loss: 0.5580 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.7689 - loss: 0.5171\n",
      "Epoch 5: val_categorical_accuracy improved from 0.75000 to 0.78000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.7705 - loss: 0.5154 - val_categorical_accuracy: 0.7800 - val_loss: 0.6175 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8058 - loss: 0.4496\n",
      "Epoch 6: val_categorical_accuracy improved from 0.78000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_GRU_Extra.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.8068 - loss: 0.4473 - val_categorical_accuracy: 0.8700 - val_loss: 0.3926 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9017 - loss: 0.2769\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.9016 - loss: 0.2768 - val_categorical_accuracy: 0.7500 - val_loss: 0.6170 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9306 - loss: 0.1891\n",
      "Epoch 8: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.9315 - loss: 0.1873 - val_categorical_accuracy: 0.8100 - val_loss: 0.4345 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9776 - loss: 0.0680\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - categorical_accuracy: 0.9776 - loss: 0.0683 - val_categorical_accuracy: 0.8400 - val_loss: 0.5089 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9748 - loss: 0.0677\n",
      "Epoch 10: val_categorical_accuracy did not improve from 0.87000\n",
      "\n",
      "Epoch 10: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.9750 - loss: 0.0674 - val_categorical_accuracy: 0.7900 - val_loss: 0.5817 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9857 - loss: 0.0392\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.9859 - loss: 0.0390 - val_categorical_accuracy: 0.8200 - val_loss: 0.5915 - learning_rate: 2.0000e-04\n",
      "Epoch 11: early stopping\n",
      "Restoring model weights from the end of the best epoch: 6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x206037f1a50>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Conv3D + GRU Model with an additional Conv3D layer\n",
    "model_conv3D_gru_extra = Sequential()\n",
    "\n",
    "# Conv3D layers\n",
    "model_conv3D_gru_extra.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                                  input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_gru_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_gru_extra.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))\n",
    "model_conv3D_gru_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Additional Conv3D layer\n",
    "model_conv3D_gru_extra.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same'))  # Extra layer\n",
    "model_conv3D_gru_extra.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Reshape for GRU layer\n",
    "model_conv3D_gru_extra.add(Reshape((-1, 16 * 16 * 64)))  # Adjust to new pooling result\n",
    "\n",
    "# GRU layer\n",
    "model_conv3D_gru_extra.add(GRU(128, return_sequences=False))\n",
    "\n",
    "# Fully connected layers\n",
    "model_conv3D_gru_extra.add(Dense(512, activation='relu'))\n",
    "model_conv3D_gru_extra.add(Dropout(0.5))\n",
    "model_conv3D_gru_extra.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_gru_extra.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# ModelCheckpoint callback to save the best model based on validation accuracy\n",
    "checkpoint_gru_extra = ModelCheckpoint(model_gru_extra_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                       save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# EarlyStopping callback to stop training when val_categorical_accuracy plateaus\n",
    "early_stopping_gru_extra = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Add callbacks to the list\n",
    "callbacks_gru_extra_list = [checkpoint_gru_extra, early_stopping_gru_extra, LR]\n",
    "\n",
    "# Train the model\n",
    "model_conv3D_gru_extra.fit(train_generator, \n",
    "                           steps_per_epoch=steps_per_epoch, \n",
    "                           epochs=num_epochs, \n",
    "                           verbose=1, \n",
    "                           callbacks=callbacks_gru_extra_list, \n",
    "                           validation_data=val_generator, \n",
    "                           validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - categorical_accuracy: 1.0000 - loss: 0.0035\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step - categorical_accuracy: 0.9366 - loss: 0.3673 \n",
      "Model: Conv3D + RNN (Extra Layer)\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.9200000166893005\n",
      "Model Size: 26 MB\n",
      "Test Inference Time: 13.1 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 4s/step - categorical_accuracy: 1.0000 - loss: 0.0010\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - categorical_accuracy: 0.8886 - loss: 0.5859 \n",
      "Model: Conv3D + LSTM (Extra Layer)\n",
      "Train Accuracy: 1.0\n",
      "Test Accuracy: 0.8700000047683716\n",
      "Model Size: 99 MB\n",
      "Test Inference Time: 16.5 sec\n",
      "==================================================\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 3s/step - categorical_accuracy: 0.9315 - loss: 0.2109\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - categorical_accuracy: 0.7922 - loss: 0.4607  \n",
      "Model: Conv3D + GRU (Extra Layer)\n",
      "Train Accuracy: 0.9155354499816895\n",
      "Test Accuracy: 0.8100000023841858\n",
      "Model Size: 75 MB\n",
      "Test Inference Time: 16.8 sec\n",
      "==================================================\n",
      "\n",
      "Performance Summary of Conv3D Models with Extra Layers:\n",
      "\n",
      "                         Model  Train Acc  Test Acc  Model Size (MB)  \\\n",
      "0                       Conv3D   1.000000      0.91               99   \n",
      "1                 Conv2D + RNN   0.989442      0.75               12   \n",
      "2                Conv2D + LSTM   1.000000      0.75               48   \n",
      "3                 Conv2D + GRU   0.996983      0.84               36   \n",
      "4                 Conv3D + RNN   0.984917      0.82               13   \n",
      "5                Conv3D + LSTM   1.000000      0.88               49   \n",
      "6                 Conv3D + GRU   1.000000      0.84               37   \n",
      "7   Conv3D + RNN (Extra Layer)   1.000000      0.92               26   \n",
      "8  Conv3D + LSTM (Extra Layer)   1.000000      0.87               99   \n",
      "9   Conv3D + GRU (Extra Layer)   0.915535      0.81               75   \n",
      "\n",
      "  Test Time (sec)  \n",
      "0            10.2  \n",
      "1            13.1  \n",
      "2            12.9  \n",
      "3            12.6  \n",
      "4            10.1  \n",
      "5            12.4  \n",
      "6            13.5  \n",
      "7            13.1  \n",
      "8            16.5  \n",
      "9            16.8  \n"
     ]
    }
   ],
   "source": [
    "# Evaluate Conv3D + RNN Model with extra layer\n",
    "evaluate_model(model_rnn_extra_filepath, 'Conv3D + RNN (Extra Layer)')\n",
    "\n",
    "# Evaluate Conv3D + LSTM Model with extra layer\n",
    "evaluate_model(model_lstm_extra_filepath, 'Conv3D + LSTM (Extra Layer)')\n",
    "\n",
    "# Evaluate Conv3D + GRU Model with extra layer\n",
    "evaluate_model(model_gru_extra_filepath, 'Conv3D + GRU (Extra Layer)')\n",
    "\n",
    "# Print the summary in a table format using pandas\n",
    "performance_summary_df = pd.DataFrame(model_accuracy_summary)\n",
    "print(\"\\nPerformance Summary of Conv3D Models with Extra Layers:\\n\")\n",
    "print(performance_summary_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second phase of experimentation, I added an additional Conv3D layer to the Conv3D + RNN, Conv3D + LSTM, and Conv3D + GRU models (models 8, 9, and 10). The objective was to improve the accuracy without significantly increasing the model size or inference time.\n",
    "\n",
    "Among these, the Conv3D + RNN (Extra Layer) model (Model 8) emerged as the best candidate. Although the Conv3D + LSTM (Extra Layer) and Conv3D + GRU (Extra Layer) models performed well in terms of accuracy, they led to a considerable increase in model size and inference time, making them less suitable for deployment in a resource-constrained environment like a webcam.\n",
    "\n",
    "Conv3D + RNN (Extra Layer) provided a reasonable accuracy improvement with minimal increase in model size and inference time. This tradeoff made it the best choice for further optimization and real-time deployment, as it balanced performance with practical constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 11\n",
    "An optimized Conv3D + RNN model with three Conv3D layers (32, 64, 32 filters) using L2 regularization and 128 SimpleRNN units. MaxPooling3D is applied after each convolution, with reduced filter sizes for efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.2512 - loss: 2.1867\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.41000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - categorical_accuracy: 0.2507 - loss: 2.1822 - val_categorical_accuracy: 0.4100 - val_loss: 1.8632 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.4228 - loss: 1.7958\n",
      "Epoch 2: val_categorical_accuracy improved from 0.41000 to 0.46000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.4240 - loss: 1.7921 - val_categorical_accuracy: 0.4600 - val_loss: 1.4995 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.5730 - loss: 1.4665\n",
      "Epoch 3: val_categorical_accuracy improved from 0.46000 to 0.61000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m113s\u001b[0m 6s/step - categorical_accuracy: 0.5739 - loss: 1.4628 - val_categorical_accuracy: 0.6100 - val_loss: 1.3352 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.6575 - loss: 1.1971\n",
      "Epoch 4: val_categorical_accuracy improved from 0.61000 to 0.75000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.6588 - loss: 1.1928 - val_categorical_accuracy: 0.7500 - val_loss: 0.9529 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.7154 - loss: 1.0029\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.75000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.7160 - loss: 1.0022 - val_categorical_accuracy: 0.6900 - val_loss: 1.0540 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8149 - loss: 0.8071\n",
      "Epoch 6: val_categorical_accuracy improved from 0.75000 to 0.76000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 6s/step - categorical_accuracy: 0.8152 - loss: 0.8054 - val_categorical_accuracy: 0.7600 - val_loss: 0.9042 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8581 - loss: 0.6551\n",
      "Epoch 7: val_categorical_accuracy improved from 0.76000 to 0.78000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - categorical_accuracy: 0.8593 - loss: 0.6529 - val_categorical_accuracy: 0.7800 - val_loss: 0.9298 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8921 - loss: 0.5934\n",
      "Epoch 8: val_categorical_accuracy improved from 0.78000 to 0.87000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 6s/step - categorical_accuracy: 0.8929 - loss: 0.5906 - val_categorical_accuracy: 0.8700 - val_loss: 0.7890 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9083 - loss: 0.4895\n",
      "Epoch 9: val_categorical_accuracy did not improve from 0.87000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m130s\u001b[0m 6s/step - categorical_accuracy: 0.9092 - loss: 0.4886 - val_categorical_accuracy: 0.8300 - val_loss: 0.8055 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9656 - loss: 0.3965\n",
      "Epoch 10: val_categorical_accuracy improved from 0.87000 to 0.88000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m127s\u001b[0m 6s/step - categorical_accuracy: 0.9655 - loss: 0.3960 - val_categorical_accuracy: 0.8800 - val_loss: 0.6392 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9859 - loss: 0.3381\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.88000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 6s/step - categorical_accuracy: 0.9855 - loss: 0.3392 - val_categorical_accuracy: 0.7600 - val_loss: 1.1398 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9653 - loss: 0.3534\n",
      "Epoch 12: val_categorical_accuracy improved from 0.88000 to 0.90000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized2.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.9655 - loss: 0.3529 - val_categorical_accuracy: 0.9000 - val_loss: 0.6093 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9885 - loss: 0.3019\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.90000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.9884 - loss: 0.3020 - val_categorical_accuracy: 0.8500 - val_loss: 0.8253 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9888 - loss: 0.2873\n",
      "Epoch 14: val_categorical_accuracy did not improve from 0.90000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.9889 - loss: 0.2872 - val_categorical_accuracy: 0.8800 - val_loss: 0.7966 - learning_rate: 0.0010\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.9856 - loss: 0.2779\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.90000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 0.9857 - loss: 0.2777 - val_categorical_accuracy: 0.8900 - val_loss: 0.7324 - learning_rate: 0.0010\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9919 - loss: 0.2639\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.90000\n",
      "\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.9921 - loss: 0.2634 - val_categorical_accuracy: 0.8600 - val_loss: 0.8982 - learning_rate: 0.0010\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9982 - loss: 0.2434\n",
      "Epoch 17: val_categorical_accuracy did not improve from 0.90000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m126s\u001b[0m 6s/step - categorical_accuracy: 0.9982 - loss: 0.2433 - val_categorical_accuracy: 0.8700 - val_loss: 0.6606 - learning_rate: 2.0000e-04\n",
      "Epoch 17: early stopping\n",
      "Restoring model weights from the end of the best epoch: 12.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x20630fecd00>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimized Conv3D + RNN Model with Dropout and L2 Regularization\n",
    "model_conv3D_rnn_optimized = Sequential()\n",
    "\n",
    "# Conv3D layers with reduced filters and L2 regularization\n",
    "model_conv3D_rnn_optimized.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                                      kernel_regularizer=l2(0.001), \n",
    "                                      input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_rnn_optimized.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Second Conv3D layer\n",
    "model_conv3D_rnn_optimized.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same',\n",
    "                                      kernel_regularizer=l2(0.001)))\n",
    "model_conv3D_rnn_optimized.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Additional Conv3D layer with reduced filters\n",
    "model_conv3D_rnn_optimized.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same',\n",
    "                                      kernel_regularizer=l2(0.001)))  # Reduced to 32 filters\n",
    "model_conv3D_rnn_optimized.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "\n",
    "# Reshape for RNN layer\n",
    "model_conv3D_rnn_optimized.add(Reshape((-1, 16 * 16 * 32)))\n",
    "\n",
    "# RNN layer with L2 regularization\n",
    "model_conv3D_rnn_optimized.add(SimpleRNN(128, return_sequences=False, kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Fully connected layers with Dropout and L2 regularization\n",
    "model_conv3D_rnn_optimized.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model_conv3D_rnn_optimized.add(Dropout(0.5))\n",
    "model_conv3D_rnn_optimized.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_rnn_optimized.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Checkpoint for saving the best model\n",
    "checkpoint_rnn_optimized = ModelCheckpoint(model_rnn_optimized1_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                           save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# EarlyStopping callback to stop training when val_categorical_accuracy plateaus\n",
    "early_stopping_rnn_optimized = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# ReduceLROnPlateau callback to reduce learning rate when validation loss plateaus\n",
    "LR = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, verbose=1, mode='auto', min_lr=1e-6)\n",
    "\n",
    "# Train the model with early stopping\n",
    "model_conv3D_rnn_optimized.fit(train_generator, \n",
    "                               steps_per_epoch=steps_per_epoch, \n",
    "                               epochs=num_epochs, \n",
    "                               verbose=1, \n",
    "                               callbacks=[checkpoint_rnn_optimized, early_stopping_rnn_optimized, LR], \n",
    "                               validation_data=val_generator, \n",
    "                               validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 4s/step - categorical_accuracy: 0.9984 - loss: 0.2790\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3s/step - categorical_accuracy: 0.9047 - loss: 0.5970 \n",
      "Model: Conv3D + RNN (Optimized)1\n",
      "Train Accuracy: 0.9969834089279175\n",
      "Test Accuracy: 0.8899999856948853\n",
      "Model Size: 13 MB\n",
      "Test Inference Time: 13.6 sec\n",
      "==================================================\n",
      "\n",
      "Updated Performance Summary with Conv3D + RNN (Optimized)1 Model:\n",
      "\n",
      "                          Model  Train Acc  Test Acc  Model Size (MB)  \\\n",
      "0                        Conv3D   1.000000      0.91               99   \n",
      "1                  Conv2D + RNN   0.989442      0.75               12   \n",
      "2                 Conv2D + LSTM   1.000000      0.75               48   \n",
      "3                  Conv2D + GRU   0.996983      0.84               36   \n",
      "4                  Conv3D + RNN   0.984917      0.82               13   \n",
      "5                 Conv3D + LSTM   1.000000      0.88               49   \n",
      "6                  Conv3D + GRU   1.000000      0.84               37   \n",
      "7    Conv3D + RNN (Extra Layer)   1.000000      0.92               26   \n",
      "8   Conv3D + LSTM (Extra Layer)   1.000000      0.87               99   \n",
      "9    Conv3D + GRU (Extra Layer)   0.915535      0.81               75   \n",
      "10    Conv3D + RNN (Optimized)1   0.996983      0.89               13   \n",
      "\n",
      "   Test Time (sec)  \n",
      "0             10.2  \n",
      "1             13.1  \n",
      "2             12.9  \n",
      "3             12.6  \n",
      "4             10.1  \n",
      "5             12.4  \n",
      "6             13.5  \n",
      "7             13.1  \n",
      "8             16.5  \n",
      "9             16.8  \n",
      "10            13.6  \n"
     ]
    }
   ],
   "source": [
    "# Evaluate the optimized Conv3D + RNN Model\n",
    "evaluate_model(model_rnn_optimized1_filepath, 'Conv3D + RNN (Optimized)1')\n",
    "\n",
    "# Print the updated performance summary in a table format using pandas\n",
    "performance_summary_df = pd.DataFrame(model_accuracy_summary)\n",
    "print(\"\\nUpdated Performance Summary with Conv3D + RNN (Optimized)1 Model:\\n\")\n",
    "print(performance_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 12\n",
    "A further optimized Conv3D + RNN model with two Conv3D layers (32, 64 filters) and 128 SimpleRNN units. Increased dropout (0.4) is added to reduce overfitting, and L2 regularization is applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.2554 - loss: 2.2734\n",
      "Epoch 1: val_categorical_accuracy improved from -inf to 0.19000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m128s\u001b[0m 6s/step - categorical_accuracy: 0.2546 - loss: 2.2720 - val_categorical_accuracy: 0.1900 - val_loss: 2.1025 - learning_rate: 0.0010\n",
      "Epoch 2/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.1784 - loss: 2.1454\n",
      "Epoch 2: val_categorical_accuracy improved from 0.19000 to 0.21000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m125s\u001b[0m 6s/step - categorical_accuracy: 0.1797 - loss: 2.1444 - val_categorical_accuracy: 0.2100 - val_loss: 2.0433 - learning_rate: 0.0010\n",
      "Epoch 3/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.2463 - loss: 2.0492\n",
      "Epoch 3: val_categorical_accuracy improved from 0.21000 to 0.40000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.2472 - loss: 2.0475 - val_categorical_accuracy: 0.4000 - val_loss: 1.8738 - learning_rate: 0.0010\n",
      "Epoch 4/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.3237 - loss: 1.8691\n",
      "Epoch 4: val_categorical_accuracy improved from 0.40000 to 0.58000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m124s\u001b[0m 6s/step - categorical_accuracy: 0.3260 - loss: 1.8656 - val_categorical_accuracy: 0.5800 - val_loss: 1.5748 - learning_rate: 0.0010\n",
      "Epoch 5/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.4723 - loss: 1.6143\n",
      "Epoch 5: val_categorical_accuracy did not improve from 0.58000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m121s\u001b[0m 6s/step - categorical_accuracy: 0.4726 - loss: 1.6135 - val_categorical_accuracy: 0.5300 - val_loss: 1.4936 - learning_rate: 0.0010\n",
      "Epoch 6/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.5497 - loss: 1.4597\n",
      "Epoch 6: val_categorical_accuracy improved from 0.58000 to 0.64000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 6s/step - categorical_accuracy: 0.5512 - loss: 1.4560 - val_categorical_accuracy: 0.6400 - val_loss: 1.3432 - learning_rate: 0.0010\n",
      "Epoch 7/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.6526 - loss: 1.2437\n",
      "Epoch 7: val_categorical_accuracy did not improve from 0.64000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.6547 - loss: 1.2397 - val_categorical_accuracy: 0.6300 - val_loss: 1.2392 - learning_rate: 0.0010\n",
      "Epoch 8/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.7045 - loss: 1.1318\n",
      "Epoch 8: val_categorical_accuracy improved from 0.64000 to 0.71000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.7065 - loss: 1.1285 - val_categorical_accuracy: 0.7100 - val_loss: 1.1134 - learning_rate: 0.0010\n",
      "Epoch 9/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.8093 - loss: 0.9829\n",
      "Epoch 9: val_categorical_accuracy improved from 0.71000 to 0.73000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 0.8097 - loss: 0.9808 - val_categorical_accuracy: 0.7300 - val_loss: 1.1650 - learning_rate: 0.0010\n",
      "Epoch 10/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.8294 - loss: 0.8417\n",
      "Epoch 10: val_categorical_accuracy improved from 0.73000 to 0.78000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m120s\u001b[0m 6s/step - categorical_accuracy: 0.8303 - loss: 0.8396 - val_categorical_accuracy: 0.7800 - val_loss: 1.0622 - learning_rate: 0.0010\n",
      "Epoch 11/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.8921 - loss: 0.7106\n",
      "Epoch 11: val_categorical_accuracy did not improve from 0.78000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 0.8925 - loss: 0.7099 - val_categorical_accuracy: 0.7500 - val_loss: 1.0734 - learning_rate: 0.0010\n",
      "Epoch 12/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.9139 - loss: 0.6484\n",
      "Epoch 12: val_categorical_accuracy did not improve from 0.78000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m117s\u001b[0m 6s/step - categorical_accuracy: 0.9149 - loss: 0.6463 - val_categorical_accuracy: 0.7800 - val_loss: 1.1132 - learning_rate: 0.0010\n",
      "Epoch 13/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.9485 - loss: 0.5592\n",
      "Epoch 13: val_categorical_accuracy did not improve from 0.78000\n",
      "\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 6s/step - categorical_accuracy: 0.9492 - loss: 0.5576 - val_categorical_accuracy: 0.7300 - val_loss: 1.1255 - learning_rate: 0.0010\n",
      "Epoch 14/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9840 - loss: 0.4658\n",
      "Epoch 14: val_categorical_accuracy improved from 0.78000 to 0.84000, saving model to C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Optimized1.keras\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m122s\u001b[0m 6s/step - categorical_accuracy: 0.9841 - loss: 0.4651 - val_categorical_accuracy: 0.8400 - val_loss: 1.0347 - learning_rate: 2.0000e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9962 - loss: 0.4256\n",
      "Epoch 15: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.9962 - loss: 0.4253 - val_categorical_accuracy: 0.7900 - val_loss: 0.9908 - learning_rate: 2.0000e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 0.9944 - loss: 0.4134\n",
      "Epoch 16: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m115s\u001b[0m 6s/step - categorical_accuracy: 0.9945 - loss: 0.4131 - val_categorical_accuracy: 0.8200 - val_loss: 1.0177 - learning_rate: 2.0000e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6s/step - categorical_accuracy: 0.9963 - loss: 0.3955\n",
      "Epoch 17: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m119s\u001b[0m 6s/step - categorical_accuracy: 0.9963 - loss: 0.3952 - val_categorical_accuracy: 0.8000 - val_loss: 0.9939 - learning_rate: 2.0000e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 1.0000 - loss: 0.3858\n",
      "Epoch 18: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m116s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.3856 - val_categorical_accuracy: 0.8300 - val_loss: 0.9884 - learning_rate: 2.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5s/step - categorical_accuracy: 1.0000 - loss: 0.3749\n",
      "Epoch 19: val_categorical_accuracy did not improve from 0.84000\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m118s\u001b[0m 6s/step - categorical_accuracy: 1.0000 - loss: 0.3747 - val_categorical_accuracy: 0.8200 - val_loss: 0.9882 - learning_rate: 2.0000e-04\n",
      "Epoch 19: early stopping\n",
      "Restoring model weights from the end of the best epoch: 14.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2065f4330a0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define callbacks for learning rate reduction and early stopping\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1, min_lr=1e-6)\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_categorical_accuracy', patience=5, verbose=1, restore_best_weights=True)\n",
    "\n",
    "# Optimized Conv3D + RNN Model with Dropout and L2 Regularization\n",
    "model_conv3D_rnn_optimized = Sequential()\n",
    "\n",
    "# Conv3D layers with L2 regularization and increased dropout\n",
    "model_conv3D_rnn_optimized.add(Conv3D(32, (3, 3, 3), activation='relu', padding='same', \n",
    "                                      kernel_regularizer=l2(0.001), \n",
    "                                      input_shape=(num_frames, img_height, img_width, 3)))\n",
    "model_conv3D_rnn_optimized.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_rnn_optimized.add(Dropout(0.4))  # Increased dropout to prevent overfitting\n",
    "\n",
    "# Second Conv3D layer\n",
    "model_conv3D_rnn_optimized.add(Conv3D(64, (3, 3, 3), activation='relu', padding='same', kernel_regularizer=l2(0.001)))\n",
    "model_conv3D_rnn_optimized.add(MaxPooling3D(pool_size=(2, 2, 2)))\n",
    "model_conv3D_rnn_optimized.add(Dropout(0.4))\n",
    "\n",
    "# Reshape for RNN layer\n",
    "model_conv3D_rnn_optimized.add(Reshape((-1, 16 * 16 * 64)))\n",
    "\n",
    "# RNN layer with L2 regularization\n",
    "model_conv3D_rnn_optimized.add(SimpleRNN(128, return_sequences=False, kernel_regularizer=l2(0.001)))\n",
    "\n",
    "# Fully connected layers with Dropout and L2 regularization\n",
    "model_conv3D_rnn_optimized.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "model_conv3D_rnn_optimized.add(Dropout(0.5))\n",
    "model_conv3D_rnn_optimized.add(Dense(5, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model_conv3D_rnn_optimized.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "# Checkpoint to save the best model\n",
    "checkpoint_rnn_optimized = ModelCheckpoint(model_rnn_optimized2_filepath, monitor='val_categorical_accuracy', verbose=1, \n",
    "                                           save_best_only=True, save_weights_only=False, mode='max')\n",
    "\n",
    "# Train the model with additional callbacks for reducing learning rate and early stopping\n",
    "model_conv3D_rnn_optimized.fit(train_generator, \n",
    "                               steps_per_epoch=steps_per_epoch, \n",
    "                               epochs=num_epochs, \n",
    "                               verbose=1, \n",
    "                               callbacks=[checkpoint_rnn_optimized, reduce_lr, early_stopping], \n",
    "                               validation_data=val_generator, \n",
    "                               validation_steps=validation_steps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3s/step - categorical_accuracy: 0.9952 - loss: 0.4133\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2s/step - categorical_accuracy: 0.8434 - loss: 1.0602  \n",
      "Model: Conv3D + RNN (Optimized)2\n",
      "Train Accuracy: 0.9954751133918762\n",
      "Test Accuracy: 0.8299999833106995\n",
      "Model Size: 25 MB\n",
      "Test Inference Time: 15.8 sec\n",
      "==================================================\n",
      "\n",
      "Updated Performance Summary with Conv3D + RNN (Optimized)2 Model:\n",
      "\n",
      "                          Model  Train Acc  Test Acc  Model Size (MB)  \\\n",
      "0                        Conv3D   1.000000      0.91               99   \n",
      "1                  Conv2D + RNN   0.989442      0.75               12   \n",
      "2                 Conv2D + LSTM   1.000000      0.75               48   \n",
      "3                  Conv2D + GRU   0.996983      0.84               36   \n",
      "4                  Conv3D + RNN   0.984917      0.82               13   \n",
      "5                 Conv3D + LSTM   1.000000      0.88               49   \n",
      "6                  Conv3D + GRU   1.000000      0.84               37   \n",
      "7    Conv3D + RNN (Extra Layer)   1.000000      0.92               26   \n",
      "8   Conv3D + LSTM (Extra Layer)   1.000000      0.87               99   \n",
      "9    Conv3D + GRU (Extra Layer)   0.915535      0.81               75   \n",
      "10    Conv3D + RNN (Optimized)1   0.996983      0.89               13   \n",
      "11    Conv3D + RNN (Optimized)2   0.995475      0.83               25   \n",
      "\n",
      "   Test Time (sec)  \n",
      "0             10.2  \n",
      "1             13.1  \n",
      "2             12.9  \n",
      "3             12.6  \n",
      "4             10.1  \n",
      "5             12.4  \n",
      "6             13.5  \n",
      "7             13.1  \n",
      "8             16.5  \n",
      "9             16.8  \n",
      "10            13.6  \n",
      "11            15.8  \n"
     ]
    }
   ],
   "source": [
    "# Evaluate the optimized Conv3D + RNN Model\n",
    "evaluate_model(model_rnn_optimized2_filepath, 'Conv3D + RNN (Optimized)2')\n",
    "\n",
    "# Print the updated performance summary in a table format using pandas\n",
    "performance_summary_df = pd.DataFrame(model_accuracy_summary)\n",
    "print(\"\\nUpdated Performance Summary with Conv3D + RNN (Optimized)2 Model:\\n\")\n",
    "print(performance_summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved in .h5 format at C:\\Users\\Vinay Joshi\\Documents\\PGD AI\\5. Gesture Recognition Project\\Bestmodels\\BestModelConv3D_RNN_Extra.keras.h5\n"
     ]
    }
   ],
   "source": [
    "# Load the best model in .keras format (model no. 8)\n",
    "best_model_rnn_extra_keras = load_model(model_rnn_extra_filepath)\n",
    "\n",
    "# Save it as .h5 format\n",
    "best_model_rnn_extra_keras.save(model_rnn_extra_filepath + '.h5')\n",
    "\n",
    "print(f\"Model saved in .h5 format at {model_rnn_extra_filepath + '.h5'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 3s/step - categorical_accuracy: 0.9149 - loss: 0.4913\n",
      "Test Loss: 0.45276182889938354\n",
      "Test Accuracy: 0.9200000166893005\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model in .h5 format\n",
    "best_model_rnn_extra_h5 = load_model(model_rnn_extra_filepath + '.h5')\n",
    "\n",
    "# Evaluate on test data (validation generator used as a substitute for test data here)\n",
    "test_loss, test_accuracy = best_model_rnn_extra_h5.evaluate(val_generator, steps=validation_steps, verbose=1)\n",
    "\n",
    "# Print the test results\n",
    "print(f\"Test Loss: {test_loss}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
